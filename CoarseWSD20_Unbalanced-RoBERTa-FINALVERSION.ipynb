{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suspected-character",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "import logging\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "small-rendering",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.7.10\n",
      "IPython version      : 7.20.0\n",
      "\n",
      "numpy       : 1.19.2\n",
      "pandas      : 1.2.2\n",
      "torch       : 1.8.1+cu111\n",
      "transformers: 4.4.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "postal-passage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "#device = 'cpu'\n",
    "\n",
    "device\n",
    "\n",
    "if device == 'cuda':\n",
    "    gpu_server = True # in case we run the notebook on Colab, set it to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "unique-intensity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comfortable-charity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sonic-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "if gpu_server == True:\n",
    "    coarseWSD20    = pd.read_csv('data/data_final_unbalanced.csv')\n",
    "else:\n",
    "    coarseWSD20    = pd.read_csv('/content/drive/MyDrive/semcor_belli/senseval_2.csv')\n",
    "    #senseval_3 = pd.read_csv('/content/drive/MyDrive/semcor_belli/senseval_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "filled-gender",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent</th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the scottish ell was equivalent to : scottish ...</td>\n",
       "      <td>34</td>\n",
       "      <td>yard_yard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the inch , foot , and yard evolved from these ...</td>\n",
       "      <td>6</td>\n",
       "      <td>yard_yard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>samuel also returned 63 punt for 673 yard , fo...</td>\n",
       "      <td>7</td>\n",
       "      <td>yard_yard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shaped roughly like a right triangle , the cam...</td>\n",
       "      <td>14</td>\n",
       "      <td>yard_yard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matthews records a playing area of 200 yard , ...</td>\n",
       "      <td>7</td>\n",
       "      <td>yard_yard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sent  idx      class\n",
       "0  the scottish ell was equivalent to : scottish ...   34  yard_yard\n",
       "1  the inch , foot , and yard evolved from these ...    6  yard_yard\n",
       "2  samuel also returned 63 punt for 673 yard , fo...    7  yard_yard\n",
       "3  shaped roughly like a right triangle , the cam...   14  yard_yard\n",
       "4  matthews records a playing area of 200 yard , ...    7  yard_yard"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarseWSD20.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "different-cycle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33566, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarseWSD20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "therapeutic-motel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pitcher_pitcher                     9209\n",
       "java_java                           3821\n",
       "bass_bass_guitar                    3361\n",
       "java_java_(programming_language)    2612\n",
       "apple_apple_inc.                    2100\n",
       "bank_bank                           1494\n",
       "apple_apple                         1290\n",
       "bass_bass_(voice_type)               907\n",
       "spring_spring_(hydrology)            752\n",
       "spring_spring_(season)               537\n",
       "seal_pinniped                        436\n",
       "arm_arm_architecture                 432\n",
       "bow_bow_(ship)                       383\n",
       "seal_seal_(emblem)                   379\n",
       "seal_seal_(musician)                 373\n",
       "square_square                        367\n",
       "bass_double_bass                     296\n",
       "club_club                            294\n",
       "crane_crane_(machine)                292\n",
       "bow_bow_and_arrow                    257\n",
       "pound_pound_(mass)                   247\n",
       "chair_chairman                       244\n",
       "deck_deck_(ship)                     244\n",
       "crane_crane_(bird)                   237\n",
       "spring_spring_(device)               232\n",
       "square_square_(company)              229\n",
       "mole_mole_(animal)                   225\n",
       "club_nightclub                       221\n",
       "yard_yard                            182\n",
       "mole_mole_(espionage)                164\n",
       "chair_chair                          157\n",
       "arm_arm                              155\n",
       "hood_hood_(comics)                   152\n",
       "mole_mole_(unit)                     150\n",
       "trunk_trunk_(botany)                 140\n",
       "bow_bow_(music)                       98\n",
       "square_town_square                    85\n",
       "digit_numerical_digit                 80\n",
       "mole_mole_sauce                       76\n",
       "club_club_(weapon)                    75\n",
       "mole_mole_(architecture)              71\n",
       "bank_bank_(geography)                 68\n",
       "hood_hood_(vehicle)                   55\n",
       "trunk_trunk_(automobile)              52\n",
       "seal_seal_(mechanical)                50\n",
       "trunk_trunk_(anatomy)                 49\n",
       "hood_hood_(headgear)                  46\n",
       "pound_pound_(currency)                36\n",
       "yard_yard_(sailing)                   34\n",
       "square_square_number                  34\n",
       "pitcher_pitcher_(container)           31\n",
       "digit_digit_(anatomy)                 30\n",
       "deck_deck_(building)                  25\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarseWSD20['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prepared-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt #For regular expressions\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "passive-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of English Contractions\n",
    "contractions_dict = { \"ain't\": \"are not\",\"aren't\": \"are not\",   \n",
    "                     \"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'s\":\"is\",\"'m\":\"am\",\"'re\":\"are\",\n",
    "                     \"'cause\": \"because\",\"could've\": \"could have\",\"couldn't\": \"could not\",\n",
    "                     \"couldn't've\": \"could not have\", \"didn't\": \"did not\",\"doesn't\": \"does not\",\n",
    "                     \"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\n",
    "                     \"hasn't\": \"has not\",\"haven't\": \"have not\",\"he'd\": \"he would\",\n",
    "                     \"he'd've\": \"he would have\",\"he'll\": \"he will\", \"he'll've\": \"he will have\",\n",
    "                     \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\n",
    "                     \"I'd\": \"I would\", \"I'd've\": \"I would have\",\"I'll\": \"I will\",\n",
    "                     \"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\", \"isn't\": \"is not\",\n",
    "                     \"it'd\": \"it would\",\"it'd've\": \"it would have\",\"it'll\": \"it will\",\n",
    "                     \"it'll've\": \"it will have\", \"let's\": \"let us\",\"ma'am\": \"madam\",\n",
    "                     \"mayn't\": \"may not\",\"might've\": \"might have\",\"mightn't\": \"might not\", \n",
    "                     \"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\n",
    "                     \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                     \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\n",
    "                     \"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n",
    "                     \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\n",
    "                     \"she'll\": \"she will\", \"she'll've\": \"she will have\",\"should've\": \"should have\",\n",
    "                     \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\"so've\": \"so have\",\n",
    "                     \"that'd\": \"that would\",\"that'd've\": \"that would have\", \"there'd\": \"there would\",\n",
    "                     \"there'd've\": \"there would have\", \"they'd\": \"they would\",\n",
    "                     \"they'd've\": \"they would have\",\"they'll\": \"they will\",\n",
    "                     \"they'll've\": \"they will have\", \"they're\": \"they are\",\"they've\": \"they have\",\n",
    "                     \"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we would\",\n",
    "                     \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\n",
    "                     \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\",\"what'll\": \"what will\",\n",
    "                     \"what'll've\": \"what will have\",\"what're\": \"what are\", \"what've\": \"what have\",\n",
    "                     \"when've\": \"when have\",\"where'd\": \"where did\", \"where've\": \"where have\",\n",
    "                     \"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who've\": \"who have\",\n",
    "                     \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\n",
    "                     \"won't've\": \"will not have\", \"would've\": \"would have\",\"wouldn't\": \"would not\",\n",
    "                     \"wouldn't've\": \"would not have\",\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                     \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                     \"y'all've\": \"you all have\", \"you'd\": \"you would\",\"you'd've\": \"you would have\",\n",
    "                     \"you'll\": \"you will\",\"you'll've\": \"you will have\", \"you're\": \"you are\",\n",
    "                     \"you've\": \"you have\",\"'scuse\":\" excuse\" }\n",
    "\n",
    "# Regular expression for finding contractions\n",
    "contractions_re=re.compile('(%s)' % '|'.join(contractions_dict.keys()))\n",
    "\n",
    "# Function for expanding contractions\n",
    "def expand_contractions(text,contractions_dict=contractions_dict):\n",
    "  def replace(match):\n",
    "    return contractions_dict[match.group(0)]\n",
    "  return contractions_re.sub(replace, text)\n",
    "\n",
    "# Expanding Contractions in the plots\n",
    "coarseWSD20['sent']=coarseWSD20['sent'].apply(lambda x:expand_contractions(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "driven-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "coarseWSD20['sent']=coarseWSD20['sent'].apply(lambda x: x.lower())\n",
    "coarseWSD20['sent']=coarseWSD20['sent'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n",
    "coarseWSD20['sent']=coarseWSD20['sent'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hydraulic-pacific",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [len(sent) for sent in coarseWSD20['sent']]\n",
    "pd.Series(sentences).hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "separate-transsexual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sent       object\n",
       "idx         int64\n",
       "class    category\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coarseWSD20['class'] = coarseWSD20['class'].astype('category')\n",
    "coarseWSD20.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "classified-daily",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = sens_df.join(pd.DataFrame(labels.toarray(), columns=enc.categories_))\n",
    "coarseWSD20[\"target\"] = coarseWSD20[\"class\"].cat.codes\n",
    "#data\n",
    "new_data = pd.DataFrame()\n",
    "new_data['text'] = coarseWSD20[\"sent\"]\n",
    "new_data['labels'] = coarseWSD20[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "resident-input",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 52)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_class = new_data['labels'].min()\n",
    "max_class = new_data['labels'].max()\n",
    "min_class, max_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "clean-dating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the scottish ell was equivalent to  scottish m...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the inch  foot  and yard evolved from these un...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>samuel also returned  punt for  yard  for an a...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>shaped roughly like a right triangle  the camp...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matthews records a playing area of  yard  with...</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  labels\n",
       "0  the scottish ell was equivalent to  scottish m...      51\n",
       "1  the inch  foot  and yard evolved from these un...      51\n",
       "2  samuel also returned  punt for  yard  for an a...      51\n",
       "3  shaped roughly like a right triangle  the camp...      51\n",
       "4  matthews records a playing area of  yard  with...      51"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "involved-light",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33    9209\n",
       "26    3821\n",
       "7     3361\n",
       "27    2612\n",
       "1     2100\n",
       "4     1494\n",
       "0     1290\n",
       "6      907\n",
       "42     752\n",
       "43     537\n",
       "37     436\n",
       "3      432\n",
       "10     383\n",
       "38     379\n",
       "40     373\n",
       "44     367\n",
       "8      296\n",
       "14     294\n",
       "18     292\n",
       "11     257\n",
       "36     247\n",
       "20     244\n",
       "13     244\n",
       "17     237\n",
       "41     232\n",
       "45     229\n",
       "28     225\n",
       "16     221\n",
       "51     182\n",
       "30     164\n",
       "12     157\n",
       "2      155\n",
       "23     152\n",
       "31     150\n",
       "50     140\n",
       "9       98\n",
       "47      85\n",
       "22      80\n",
       "32      76\n",
       "15      75\n",
       "29      71\n",
       "5       68\n",
       "25      55\n",
       "49      52\n",
       "39      50\n",
       "48      49\n",
       "24      46\n",
       "35      36\n",
       "46      34\n",
       "52      34\n",
       "34      31\n",
       "21      30\n",
       "19      25\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-boost",
   "metadata": {},
   "source": [
    "## BERT BASE UNCASED + 2 LAYER MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abstract-assist",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "accessory-salem",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "antique-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rental-northwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.1'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "differential-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 200\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "TEST_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 4\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "driving-boston",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.targets = self.data.labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation = True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.long) #long\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "polish-spoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (33566, 2)\n",
      "TRAIN Dataset: (25174, 2)\n",
      "TEST Dataset: (5035, 2)\n",
      "VALIDATION Dataset: (3357, 2)\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.75\n",
    "test_size  = 0.60\n",
    "train_data = new_data.sample(frac=train_size,random_state=200)\n",
    "eval_data  = new_data.drop(train_data.index)#.reset_index(drop=True)\n",
    "test_data  = eval_data.sample(frac=test_size,random_state=200)\n",
    "val_data   = eval_data.drop(test_data.index)\n",
    "\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "test_data  = test_data.reset_index(drop=True)\n",
    "val_data   = val_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(new_data.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "print(\"VALIDATION Dataset: {}\".format(val_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "collect-fraud",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set   = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\n",
    "testing_set    = MultiLabelDataset(test_data, tokenizer, MAX_LEN)\n",
    "validating_set = MultiLabelDataset(val_data, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "national-country",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': TEST_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "val_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader   = DataLoader(training_set, **train_params)\n",
    "testing_loader    = DataLoader(testing_set, **test_params)\n",
    "validating_loader = DataLoader(validating_set, **val_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "exterior-antique",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RoBERTaClass(\n",
       "  (l1): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=53, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaModel\n",
    "\n",
    "class RoBERTaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RoBERTaClass, self).__init__()\n",
    "        self.l1 = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        #self.classifier_hid = torch.nn.Linear(768, 1512)\n",
    "        #self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.classifier = torch.nn.Linear(768, 53)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        #pooler = self.classifier_hid(pooler)\n",
    "        #pooler = self.dropout(pooler)\n",
    "        #pooler = torch.nn.ReLU()(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\n",
    "\n",
    "model = RoBERTaClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "objective-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples): #scheduler,\n",
    "    model = model.train() \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for _,data in enumerate(data_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "        \n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, dim=1) #max\n",
    "        loss = loss_fn(outputs, targets) # get loss\n",
    "        \n",
    "        correct_predictions += torch.sum(preds==targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "collective-designer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval() #Evaluation mode\n",
    "    \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _,data in enumerate(data_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, dim=1)\n",
    "            \n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            correct_predictions += torch.sum(preds==targets)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adverse-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "def train_model(model, data_loaders_train, data_loader_test, train_sizes, test_sizes, device, n_epochs):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.01) #0.01\n",
    "    loss_fn   = torch.nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    history = defaultdict(list)\n",
    "    best_accuracy = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{n_epochs}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        train_acc, train_loss = train_epoch(model, training_loader, loss_fn, \n",
    "                                            optimizer, device, scheduler, train_data.shape[0]) #scheduler,\n",
    "        \n",
    "        print(f'Train loss {train_loss}, accuracy {train_acc}') #accuracy {train_acc}\n",
    "        \n",
    "        val_acc, val_loss = eval_model(model, testing_loader, loss_fn, device, test_data.shape[0])\n",
    "        \n",
    "        print(f'Val loss {val_loss}, accuracy {val_acc}') # accuracy {val_acc}\n",
    "        print()\n",
    "        \n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        if val_acc > best_accuracy:\n",
    "            torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "            best_accuracy = val_acc\n",
    "            \n",
    "    print(f'Best val accuracy: {best_accuracy}')\n",
    "    \n",
    "    model.load_state_dict(torch.load('best_model_state.bin'))\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "nuclear-communication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n",
      "Train loss 0.45052862876948074, accuracy 0.9026773655358704\n",
      "Val loss 0.14291753183592243, accuracy 0.9600794438927508\n",
      "\n",
      "Epoch 2/20\n",
      "----------\n",
      "Train loss 0.1315290273691418, accuracy 0.9637721458647811\n",
      "Val loss 0.1130217493156486, accuracy 0.9686196623634558\n",
      "\n",
      "Epoch 3/20\n",
      "----------\n",
      "Train loss 0.09849836540825238, accuracy 0.9716374036704537\n",
      "Val loss 0.11112117654061657, accuracy 0.965441906653426\n",
      "\n",
      "Epoch 4/20\n",
      "----------\n",
      "Train loss 0.07844653745989512, accuracy 0.977238420592675\n",
      "Val loss 0.1457151995760244, accuracy 0.9630585898709036\n",
      "\n",
      "Epoch 5/20\n",
      "----------\n",
      "Train loss 0.06728898113678354, accuracy 0.9816477317867641\n",
      "Val loss 0.1322965957122627, accuracy 0.9680238331678253\n",
      "\n",
      "Epoch 6/20\n",
      "----------\n",
      "Train loss 0.04737973024179971, accuracy 0.986414554699293\n",
      "Val loss 0.11707296466762537, accuracy 0.9715988083416087\n",
      "\n",
      "Epoch 7/20\n",
      "----------\n",
      "Train loss 0.03835186315873554, accuracy 0.9892349249225392\n",
      "Val loss 0.1109604528879721, accuracy 0.9733862959285005\n",
      "\n",
      "Epoch 8/20\n",
      "----------\n",
      "Train loss 0.03248753478596502, accuracy 0.9912211011360929\n",
      "Val loss 0.11106603642992467, accuracy 0.9737835153922543\n",
      "\n",
      "Epoch 9/20\n",
      "----------\n",
      "Train loss 0.03088154577389474, accuracy 0.9914991658059903\n",
      "Val loss 0.11046959841313951, accuracy 0.9737835153922543\n",
      "\n",
      "Epoch 10/20\n",
      "----------\n",
      "Train loss 0.02792176324714893, accuracy 0.9923333598156828\n",
      "Val loss 0.11131255895057192, accuracy 0.9739821251241311\n",
      "\n",
      "Epoch 11/20\n",
      "----------\n",
      "Train loss 0.02806123437540132, accuracy 0.9922141892428696\n",
      "Val loss 0.11131761703743051, accuracy 0.9739821251241311\n",
      "\n",
      "Epoch 12/20\n",
      "----------\n",
      "Train loss 0.028150639229308598, accuracy 0.9924525303884961\n",
      "Val loss 0.11126827079550228, accuracy 0.9739821251241311\n",
      "\n",
      "Epoch 13/20\n",
      "----------\n",
      "Train loss 0.028387752071323907, accuracy 0.9920552951457854\n",
      "Val loss 0.11125439128198837, accuracy 0.9739821251241311\n",
      "\n",
      "Epoch 14/20\n",
      "----------\n",
      "Train loss 0.02752755008907328, accuracy 0.9928497656312069\n",
      "Val loss 0.11123745048984515, accuracy 0.9739821251241311\n",
      "\n",
      "Epoch 15/20\n",
      "----------\n",
      "Train loss 0.027459388412131337, accuracy 0.992373083339954\n",
      "Val loss 0.1112172279995129, accuracy 0.9739821251241311\n",
      "\n",
      "Epoch 16/20\n",
      "----------\n",
      "Train loss 0.027510797088829947, accuracy 0.9926114244855804\n",
      "Val loss 0.11121719220306923, accuracy 0.9739821251241311\n",
      "\n",
      "Epoch 17/20\n",
      "----------\n",
      "Train loss 0.027082841296014678, accuracy 0.9923333598156828\n",
      "Val loss 0.11135141981342166, accuracy 0.9739821251241311\n",
      "\n",
      "Epoch 18/20\n",
      "----------\n",
      "Train loss 0.027254806311133514, accuracy 0.992929212679749\n",
      "Val loss 0.1112172507947428, accuracy 0.9739821251241311\n",
      "\n",
      "Epoch 19/20\n",
      "----------\n",
      "Train loss 0.026083869104165155, accuracy 0.9926114244855804\n",
      "Val loss 0.11121904735260019, accuracy 0.9739821251241311\n",
      "\n",
      "Epoch 20/20\n",
      "----------\n",
      "Train loss 0.02682137960459848, accuracy 0.9921347421943275\n",
      "Val loss 0.11122989241609543, accuracy 0.9739821251241311\n",
      "\n",
      "Best val accuracy: 0.9739821251241311\n",
      "CPU times: user 2h 48min 28s, sys: 7min 35s, total: 2h 56min 3s\n",
      "Wall time: 2h 56min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "base_model, history = train_model(model, training_loader, testing_loader, train_data.shape[0], test_data.shape[0],\n",
    "                                  device, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "changed-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image as Image\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "declared-spice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_training_history(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(18,6))\n",
    "    ax1.plot(history['train_loss'], label='train loss')\n",
    "    ax1.plot(history['val_loss'], label='test loss')\n",
    "    \n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax1.set_ylim([0.00,1.00])\n",
    "    ax1.legend()\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    \n",
    "    ax2.plot(history['train_acc'], label='train accuracy')\n",
    "    ax2.plot(history['val_acc'], label='test accuracy')\n",
    "    \n",
    "    ax2.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax2.set_ylim([0.00,1.00])\n",
    "    ax2.legend()\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    \n",
    "    fig.suptitle('Training History')\n",
    "    \n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "continent-activity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "equivalent-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    "    predictions = []\n",
    "    real_values = []\n",
    "    with torch.no_grad():\n",
    "        for _,data in enumerate(data_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "            \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            \n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            predictions.extend(preds)\n",
    "            real_values.extend(targets)\n",
    "    predictions = torch.as_tensor(predictions).cpu()\n",
    "    real_values = torch.as_tensor(real_values).cpu()\n",
    "    \n",
    "    return predictions, real_values\n",
    "\n",
    "y_pred, y_test = get_predictions(model, validating_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "pretty-pitch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       129\n",
      "           1       1.00      1.00      1.00       203\n",
      "           2       1.00      1.00      1.00        17\n",
      "           3       1.00      1.00      1.00        48\n",
      "           4       1.00      0.98      0.99       154\n",
      "           5       0.50      1.00      0.67         3\n",
      "           6       0.93      0.97      0.95        71\n",
      "           7       0.94      0.96      0.95       321\n",
      "           8       0.70      0.54      0.61        39\n",
      "           9       0.80      0.80      0.80        10\n",
      "          10       1.00      0.97      0.98        30\n",
      "          11       0.93      0.96      0.95        28\n",
      "          12       0.91      0.91      0.91        11\n",
      "          13       0.96      0.96      0.96        24\n",
      "          14       0.79      0.73      0.76        30\n",
      "          15       0.88      0.78      0.82         9\n",
      "          16       0.80      0.88      0.84        32\n",
      "          17       0.97      1.00      0.98        28\n",
      "          18       1.00      0.97      0.99        37\n",
      "          19       0.67      0.40      0.50         5\n",
      "          20       0.91      0.97      0.94        30\n",
      "          21       1.00      0.67      0.80         3\n",
      "          22       1.00      1.00      1.00        11\n",
      "          23       1.00      1.00      1.00        18\n",
      "          24       1.00      1.00      1.00         3\n",
      "          25       1.00      1.00      1.00         9\n",
      "          26       1.00      1.00      1.00       383\n",
      "          27       1.00      1.00      1.00       284\n",
      "          28       0.80      0.92      0.86        13\n",
      "          29       0.83      1.00      0.91         5\n",
      "          30       0.94      0.94      0.94        18\n",
      "          31       1.00      0.96      0.98        24\n",
      "          32       1.00      0.91      0.95        11\n",
      "          33       1.00      1.00      1.00       907\n",
      "          34       1.00      1.00      1.00         2\n",
      "          35       0.50      0.67      0.57         3\n",
      "          36       0.96      0.92      0.94        24\n",
      "          37       0.97      0.97      0.97        40\n",
      "          38       0.98      0.98      0.98        43\n",
      "          39       0.88      1.00      0.93         7\n",
      "          40       0.96      0.93      0.94        27\n",
      "          41       1.00      0.96      0.98        25\n",
      "          42       0.95      0.98      0.97        63\n",
      "          43       0.96      0.94      0.95        54\n",
      "          44       0.92      1.00      0.96        47\n",
      "          45       1.00      1.00      1.00        24\n",
      "          46       1.00      0.50      0.67         2\n",
      "          47       1.00      0.75      0.86        12\n",
      "          48       0.75      0.50      0.60         6\n",
      "          49       1.00      0.80      0.89         5\n",
      "          50       0.69      0.90      0.78        10\n",
      "          51       1.00      0.93      0.96        14\n",
      "          52       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.97      3357\n",
      "   macro avg       0.91      0.90      0.90      3357\n",
      "weighted avg       0.97      0.97      0.97      3357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-packaging",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
